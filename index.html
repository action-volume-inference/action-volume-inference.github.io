<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Avi: Action from Volumetric Inference - A 3D Vision-Language-Action model for robotic manipulation through volumetric reasoning.">
  <meta name="keywords" content="Avi, 3D VLA, Vision-Language-Action, Robotics, Volumetric Inference, ShapeLLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Avi: Action from Volumetric Inference</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Avi: Action from Volumetric Inference</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Neural Information Processing 39th</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>Demo</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Avi</span> enables robots to perform complex manipulation tasks through 
        3D volumetric reasoning and natural language instructions.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">System Architecture</h2>
        <div class="content has-text-justified">
          <p>
            Avi combines stereo reconstruction, 2D segmentation (via Segment Anything), and a fine-tuned 
            3D Vision-Language Model (based on Qwen-VL and 3D VQVAE embeddings) to predict goal-conditioned 
            3D volumes. We further align these volumes using classical geometric optimization (ICP) to 
            produce interpretable, spatially grounded actions.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="box">
                <h4 class="title is-5">Input Pipeline</h4>
                <ul>
                  <li><strong>Stereo RGB-D:</strong> Real/simulated left and right RGB images with depth</li>
                  <li><strong>SAM Segmentation:</strong> Object-level segmentation for spatial grounding</li>
                  <li><strong>3D Point Cloud:</strong> Reconstructed scene representation P<sub>t</sub></li>
                </ul>
              </div>
            </div>
            <div class="column is-8">
              <div class="box">
                <h4 class="title is-5">Processing Pipeline</h4>
                <ul>
                  <li><strong>3D VQVAE:</strong> Encodes/decodes voxelized 3D shapes into discrete tokens</li>
                  <li><strong>Qwen 2.5 7B:</strong> Vision-language model with LoRA fine-tuning</li>
                  <li><strong>Location Quantization:</strong> 896 spatial tokens (X,Y,Z position + scale)</li>
                </ul>
              </div>
            </div>
            <div class="column is-8">
              <div class="box">
                <h4 class="title is-5">Output Pipeline</h4>
                <ul>
                  <li><strong>Volumetric Prediction:</strong> Next state point cloud PÌ‚<sub>t+1</sub></li>
                  <li><strong>ICP Alignment:</strong> Geometric optimization for spatial transformation</li>
                  <li><strong>Action Execution:</strong> Morphology-agnostic robot control via IK</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present Avi (Action from Volumetric Inference), a novel 3D Vision-Language-Action (VLA) 
            architecture that reframes robotic control as a problem of volumetric reasoning rather than 
            low-level policy generation. By leveraging ShapeLLM-Omni as a 3D Multi-Modal Language Model 
            and extending it with location quantization, we enable the model to interpret natural language 
            instructions and predict goal-conditioned 3D representations of the environment. These predicted 
            volumes are then aligned through geometric optimization, yielding interpretable and 
            morphology-agnostic actions.
          </p>
          <p>
            Our approach combines stereo reconstruction, 2D segmentation (via Segment Anything), and a 
            fine-tuned 3D Vision-Language Model (based on Qwen-VL and 3D VQVAE embeddings) to predict 
            goal-conditioned 3D volumes. We further align these volumes using classical geometric 
            optimization (ICP) to produce interpretable, spatially grounded actions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby2">
          <video poster="" id="toby2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Contributions</h2>
        <div class="content has-text-justified">
          <ol>
            <li>
              <strong>AVI (Action from Volumetric Inference):</strong> A novel architecture that integrates 
              a 3D Multi-Modal Language Model to infer actions through volumetric reasoning, rather than 
              directly generating action tokens. This approach shifts the focus from language-to-action 
              to language-to-geometry, enabling richer spatial grounding.
            </li>
            <li>
              <strong>Location Quantization for 3D MLLMs:</strong> A general technique for discretizing 
              spatial information that allows pretrained 3D MLLMs to generalize at the object level rather 
              than at the scene level. Unlike current state-of-the-art methods, which often overfit to 
              scene layouts, our quantization method enables modularity and reusability across different 
              3D architectures and robotics tasks.
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Architecture Overview</h3>
          <p>
            Our architecture extends the foundational 3D model, ShapeLLM-Omni, which is pretrained on 
            large-scale 3D assets and capable of handling multi-modal inputs, including text, images, 
            and 3D point clouds. We integrate Qwen-2.5 (7B), a state-of-the-art large vision-language 
            model, with ShapeLLM-Omni to inherit both powerful linguistic grounding and native 3D spatial 
            reasoning.
          </p>
          
          <h3 class="title is-4">Location Quantization</h3>
          <p>
            We extend the vocabulary by introducing dedicated position and scale tokens. Specifically, 
            we define three independent position axes: X, Y, Z âˆˆ {1,2,...,256}, each discretized into 
            256 bins, and discretize object scale into S âˆˆ {1,2,...,128}, yielding 128 scale tokens. 
            This introduces a total of 896 additional tokens for spatial context.
          </p>
          
          <h3 class="title is-4">Transformation Calculation</h3>
          <p>
            Given a prompt and current scene point cloud P<sub>t</sub>, we generate a next point cloud 
            prediction PÌ‚<sub>t+1</sub> such that: PÌ‚<sub>t+1</sub> â‰ˆ P<sub>t</sub> + Î”P where Î”P 
            represents the learned spatial change conditioned on the prompt. We then compute the 
            Iterative Closest Point (ICP) transformation to minimize alignment error.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Dataset and Training</h3>
          <p>
            We fine-tune the foundational model on robotics training data using the LIBERO Dataset, 
            which provides diverse task demonstrations within the Robosuite environment. Our experiments 
            focus on the drawer-closing task, demonstrating that Avi produces semantically consistent 
            and physically realizable manipulations from only a small number of demonstrations (50 demos).
          </p>
          
          <h3 class="title is-4">Task Performance</h3>
          <p>
            Our experiments on the drawer-closing task show successful execution across eighteen inference 
            steps. The model demonstrates the ability to generate semantically consistent and physically 
            realizable action trajectories conditioned on natural language instructions like "Close the 
            drawer with the robot."
          </p>
          
          <h3 class="title is-4">Ablation Studies</h3>
          <p>
            Through qualitative ablation studies, we demonstrate that location quantization is critical 
            for precise manipulation. In tasks requiring fine-grained control, such as pick-and-place 
            or insertion, the model must accurately predict gripper positions for correct end-effector 
            motions.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-6">
              <div class="box has-background-success-light">
                <h4 class="title is-6">With Location Quantization</h4>
                <ul>
                  <li>âœ“ Reliable spatial grounding</li>
                  <li>âœ“ Consistent end-effector alignment</li>
                  <li>âœ“ Successful precision tasks</li>
                  <li>âœ“ Robust geometric reasoning</li>
                </ul>
              </div>
            </div>
            <div class="column is-6">
              <div class="box has-background-danger-light">
                <h4 class="title is-6">Without Location Quantization</h4>
                <ul>
                  <li>âœ— Ambiguous spatial predictions</li>
                  <li>âœ— Suboptimal end-effector motions</li>
                  <li>âœ— Failed precision tasks</li>
                  <li>âœ— Poor geometric reasoning</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Technical Details</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Model Architecture</h3>
          <ul>
            <li><strong>3D VQVAE Encoder/Decoder:</strong> Maps voxelized 3D shapes into discrete latent representations</li>
            <li><strong>Qwen 2.5 7B:</strong> Vision-Language Model backbone for language understanding</li>
            <li><strong>SAM (Segment Anything Model):</strong> For object segmentation and isolation</li>
            <li><strong>LoRA Injection:</strong> Efficient parameter updates without full model retraining</li>
            <li><strong>ICP (Iterative Closest Point):</strong> Geometric optimization for spatial alignment</li>
          </ul>
          
          <h3 class="title is-4">Training Setup</h3>
          <ul>
            <li><strong>Hardware:</strong> Single NVIDIA A6000 GPU with 48GB memory</li>
            <li><strong>Dataset:</strong> LIBERO Dataset with Robosuite demonstrations</li>
            <li><strong>Regularization:</strong> Dropout (p = 0.05) for limited-data regime</li>
            <li><strong>Fine-tuning:</strong> LoRA adaptation on last K attention layers</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison with Related Work</h2>
        <div class="content has-text-justified">
          <p>
            Unlike prior Vision-Language-Action (VLA) methods that directly predict robot-specific 
            action tokens, our approach emphasizes morphology-agnostic policy: rather than outputting 
            actions, our model predicts transformed 3D point clouds from which robot-specific trajectories 
            can be computed via inverse kinematics.
          </p>
          
          <div class="table-container">
            <table class="table is-fullwidth is-striped">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Input Modality</th>
                  <th>Core Mechanism</th>
                  <th>Generates 3D Point Clouds?</th>
                  <th>No Action Tokens Needed?</th>
                </tr>
              </thead>
              <tbody>
                <tr class="has-background-success-light">
                  <td><strong>This Work (Avi)</strong></td>
                  <td>3D point clouds + language</td>
                  <td>3D MLLM predicting delta point clouds + IK</td>
                  <td>âœ“ Yes</td>
                  <td>âœ“ Yes</td>
                </tr>
                <tr>
                  <td>Robot4DGen</td>
                  <td>RGB-D video</td>
                  <td>4D video generation with multi-view constraint</td>
                  <td>âœ— No (video only)</td>
                  <td>âœ— No</td>
                </tr>
                <tr>
                  <td>Unified Video-Action (UVA)</td>
                  <td>RGB video</td>
                  <td>Joint videoâ€“action latent modeling</td>
                  <td>âœ— No</td>
                  <td>âœ— No</td>
                </tr>
                <tr>
                  <td>DP3</td>
                  <td>3D point clouds</td>
                  <td>Diffusion model over actions conditioned on 3D</td>
                  <td>Uses 3D conditioning, outputs actions</td>
                  <td>âœ— No</td>
                </tr>
                <tr>
                  <td>FP3</td>
                  <td>3D point clouds + language</td>
                  <td>Diffusion transformer policy pre-trained on 3D</td>
                  <td>âœ— No (actions directly)</td>
                  <td>âœ— No</td>
                </tr>
              </tbody>
            </table>
          </div>
          
          <p>
            This shift from language-to-action to language-to-geometry enables richer spatial grounding, 
            efficient sim-to-real transfer, and more robust reasoning in multi-object robotic environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Interactive Demo</h2>
        <div class="content has-text-justified">
          <p>
            Experience Avi's capabilities through our interactive demonstration. The system processes 
            natural language instructions and generates 3D volumetric predictions for robotic manipulation tasks.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-6">
              <div class="box">
                <h4 class="title is-5">Example Instructions</h4>
                <ul>
                  <li>"Close the drawer with the robot"</li>
                  <li>"Pick up the red block and place it on the table"</li>
                  <li>"Move the cup to the left side of the table"</li>
                  <li>"Grasp the object and rotate it 90 degrees"</li>
                </ul>
              </div>
            </div>
            <div class="column is-6">
              <div class="box">
                <h4 class="title is-5">System Response</h4>
                <ul>
                  <li>3D scene understanding and object segmentation</li>
                  <li>Volumetric reasoning and spatial planning</li>
                  <li>Geometric transformation calculation</li>
                  <li>Morphology-agnostic action execution</li>
                </ul>
              </div>
            </div>
          </div>
          
          <div class="has-text-centered" style="margin-top: 2rem;">
            <a href="./static/voxel-viewer.html" class="button is-large is-primary" target="_blank">
              <span class="icon">
                <i class="fas fa-cube"></i>
              </span>
              <span>Launch VOX Viewer</span>
            </a>
            <button class="button is-large is-info" onclick="alert('Robot control demo would be implemented here with actual robot simulation and control interfaces.')">
              <span class="icon">
                <i class="fas fa-robot"></i>
              </span>
              <span>Robot Control Demo</span>
            </button>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Future Work</h2>
        <div class="content has-text-justified">
          <p>
            In future work, we plan to extend Avi to multi-task and multi-robot settings, evaluate it 
            under real-world deployment using stereo-based 3D reconstruction pipelines, and integrate 
            reinforcement learning to further refine long-horizon planning capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-left">
          <pre><code>@article{avi2024,
  title={Avi: Action from Volumetric Inference},
  author={Research Team},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        <strong>Avi: Action from Volumetric Inference</strong> - A 3D Vision-Language-Action model for robotic manipulation.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
